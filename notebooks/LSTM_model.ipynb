{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "import transformers\n",
    "import tensorflow as ts\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0e5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33059240",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('model',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0252293",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fcee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "missing_labels = data[\"sentiment\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194ac1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of missing labels: {missing_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b34165",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = data[data['sentiment'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22bd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = ['sentiment'],inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89751f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['sentiment'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['review'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentiment labels as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['sentiment'] = le.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ea3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bc7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['sentiment'].values\n",
    "x = data['review'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669a4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Convert text to sequences of integers (word IDs)\n",
    "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "maxlen = max(len(seq) for seq in sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec035cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save tokenizer \n",
    "with open(\"tokenizer.pkl\", \"wb\") as f: pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b5123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "num_folds = 5\n",
    "vocab_size = 20000\n",
    "embedding_dim = 128\n",
    "batch_size = 16\n",
    "epochs = 3\n",
    "maxlen = 64\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(x)  # x should be a list/Series of texts\n",
    "x_pad = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_cat = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "# Define KFold\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_no = 1\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "\n",
    "for train_index, val_index in kf.split(x_pad):\n",
    "    print(f\"Training fold {fold_no}...\")\n",
    "\n",
    "    # Split the data\n",
    "    x_train, x_val = x_pad[train_index], x_pad[val_index]\n",
    "    y_train, y_val = y_cat[train_index], y_cat[val_index]\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        validation_data=(x_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    scores = model.evaluate(x_val, y_val, verbose=0)\n",
    "    print(f\"Score for fold {fold_no}: loss={scores[0]:.4f}, accuracy={scores[1]:.4f}\")\n",
    "\n",
    "    acc_per_fold.append(scores[1])\n",
    "    loss_per_fold.append(scores[0])\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "# Save the entire model \n",
    "model.save(\"my_LSTM_model\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26b260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary of results:\n",
    "print(\"Average scores for all folds:\")\n",
    "print(f\"Accuracy: {np.mean(acc_per_fold):.4f} (+/- {np.std(acc_per_fold):.4f})\")\n",
    "print(f\"Loss: {np.mean(loss_per_fold):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de481747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "text = \"The movie was amazing!\"\n",
    "\n",
    "# Tokenize using your LSTM tokenizer\n",
    "seq = tokenizer.texts_to_sequences([text])\n",
    "\n",
    "# Pad the sequence to maxlen (same as training)\n",
    "padded_seq = pad_sequences(seq, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Get predictions from LSTM model\n",
    "pred_probs = model.predict(padded_seq)  # shape: (1, num_classes)\n",
    "\n",
    "\n",
    "print(\"Predicted class:\", pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8119851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(le.classes_,le.transform(le.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a320cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "# 1) Pad validation set\n",
    "x_val_padded = pad_sequences(x_val, maxlen=maxlen, padding='post')\n",
    "\n",
    "# 2) Predict probabilities\n",
    "y_pred_probs = model.predict(x_val_padded)\n",
    "\n",
    "\n",
    "print(classification_report(y_val, y_pred, target_names=['negative', 'neutral', 'positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d6ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Confusion matrix (prettier version)\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_df = pd.DataFrame(cm, index=['negative', 'neutral', 'positive'], columns=['negative', 'neutral', 'positive'])\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.title(\"Confusion Matrix - LSTM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_macro = f1_score(y_val, y_pred, average=\"macro\")\n",
    "f1_micro = f1_score(y_val, y_pred, average=\"micro\")  # overall F1, weighted by number of samples per class\n",
    "f1_weighted = f1_score(y_val, y_pred, average=\"weighted\")  # averages per class, weighted by support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b19078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSTM Model F1 Scores:\")\n",
    "print(f\"Micro F1: {f1_micro:.4f}\")\n",
    "print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1: {f1_weighted:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
