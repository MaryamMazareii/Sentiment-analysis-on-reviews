{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import warnings\n",
    "import transformers\n",
    "import tensorflow as ts\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64535ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a6693",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a174a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('model',axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f3055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "\n",
    "missing_labels = data[\"sentiment\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23cc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'number of missing labels: {missing_labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbfabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rows = data[data['sentiment'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdab23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04132747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset = ['sentiment'],inplace=True)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda8230",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['sentiment'].isnull().sum())\n",
    "print(data['review'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ffd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentiment labels as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "data['sentiment'] = le.fit_transform(data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b78df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d7e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute token lengths for each review\n",
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "length = data['review'].apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af3dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(length.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67799f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"empty reviews: \", ((length > 0) & (length <=1)).sum())\n",
    "print(\"1-24 tokens: \", ((length > 1) & (length <=24)).sum())\n",
    "print(\"24–60 tokens: \", ((length > 24) & (length <= 64)).sum())\n",
    "print(\">60 tokens: \", (length > 60).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab947428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all reviews\n",
    "encoder = tokenizer(\n",
    "    list(data['review']),\n",
    "    truncation=True,\n",
    "    max_length=60,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80802a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fd521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input IDs and attention masks from tensors to NumPy arrays for TensorFlow compatibility\n",
    "input_ids = encoder[\"input_ids\"].numpy()    \n",
    "attention_mask =  encoder[\"attention_mask\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daa7c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486177ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data['review'].values\n",
    "y = data['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b98413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split input IDs, attention masks, and labels into training and testing sets\n",
    "x_train_ids,x_test_ids,x_train_mask,x_test_mask,y_train,y_test = train_test_split(\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fab04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_ids.shape)\n",
    "print(x_train_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b659173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFRobertaForSequenceClassification\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\n",
    "    'roberta-base',\n",
    "    num_labels = len(le.classes_),\n",
    "    force_download=True,  # forces redownload\n",
    "    resume_download=False  # don’t resume partial download\n",
    ")\n",
    "\n",
    "# Define optimizer and loss function for training\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed59329",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping   #prevents using too much space\n",
    "earlystop = EarlyStopping( \n",
    "    monitor = 'val_loss', \n",
    "    patience = 2, \n",
    "    restore_best_weights = True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    {'input_ids':x_train_ids, 'attention_mask':x_train_mask, 'labels':y_train}, \n",
    "    validation_data = ({'input_ids':x_test_ids,'attention_mask':x_test_mask},y_test), # Includes validation data for performance monitoring and early stopping to prevent overfitting\n",
    "    epochs=3,\n",
    "    batch_size=16, \n",
    "    callbacks = [earlystop] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving our model:\n",
    "\n",
    "model.save_pretrained(\"my_tf_roberta_model\")\n",
    "tokenizer.save_pretrained(\"my_tf_roberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(\n",
    "    {\n",
    "        \"input_ids\": x_test_ids,\n",
    "        \"attention_mask\": x_test_mask\n",
    "    },\n",
    "    y_test,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58614fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8d204a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference on a new example\n",
    "text = \"The Prices were amazing!\"\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"tf\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=64\n",
    ")\n",
    "\n",
    "# Get model predictions\n",
    "outputs = model(inputs)\n",
    "pred = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "\n",
    "print(\"Predicted class:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions and print classification report to evaluate model performance\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred_probs = model.predict([x_test_ids, x_test_mask]).logits\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = dict(zip(le.classes_,le.transform(le.classes_)))\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89880322",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"Negative\",\"Neutral\",\"Positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf839d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix to visualize model performance across sentiment classes\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Greens\", xticklabels=label,yticklabels=label)\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faf4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report as a dict\n",
    "report = classification_report(y_test, y_pred, target_names=label, output_dict=True)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(report).transpose()\n",
    "\n",
    "# Select only precision, recall, f1-score\n",
    "df_metrics = df[['precision', 'recall', 'f1-score']].iloc[:-3]  # drop accuracy/avg rows\n",
    "\n",
    "# Plot\n",
    "df_metrics.plot(kind='bar', figsize=(8,6))\n",
    "plt.title(\"Classification Report Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1.05)  # scores are between 0 and 1\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd57c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "f1_micro = f1_score(y_test, y_pred, average=\"micro\")\n",
    "f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0895041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BERT Model F1 Scores:\")\n",
    "print(f\"Micro F1: {f1_micro:.4f}\")\n",
    "print(f\"Macro F1: {f1_macro:.4f}\")\n",
    "print(f\"Weighted F1: {f1_weighted:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
